{"name":"A look at some Tweets from Thanksgiving 2015","tagline":"A look at some Tweets obtained during Thanksgiving 2015. Analysis done using Apache Zeppelin and the Spark interpreter","body":"# A look at some Tweets from Thanksgiving 2015\r\n\r\n## Overview\r\n***\r\n\r\nI will admit I am a huge fan of Twitter, and of the data it produces because it can holds many interesting facts, and opinions of almost any subject from people from all around the world. During Thanksgiving 2015 (November 26), while everyone was eating turkey, I fired up Spark to capture tweets containing the keyword ‘thanksgiving’.\r\n\r\nThe reason I did this work was because I was interested in exploring the tweets generated during that period of time, in particular topics such as the most common retweet, and hashtag. Moreover, I wanted to try Apache Zeppelin, a web-based notebook (similar to iPython or Jupyter) for interactive data analytics.\r\n\r\n## The data\r\n***\r\nThe dataset used is made of 177955 tweets obtained on November 26, 2015.\r\n\r\n## Platforms used\r\n- Apache Zeppelin and the Spark interpreter\r\n- Spark Streaming\r\n- Pig\r\n\r\n## Report\r\n\r\n### Obtaining the data\r\nThe data used for this work was obtained using Spark Streaming, and its Twitter library. The script written captured only the text component of the tweet, in other word, just the tweet itself. After an hour or so of capturing tweets, I ended up with a directory made of many subdirectories that had the tweets. Because of this, a Pig script was written to transfer the content of all these files into a single one. Both of these scripts are on the GitHub repository.\r\n\r\n### The result\r\n\r\nOnce the data was in the wanted format, it was loaded into Zeppelin. Keep in mind that I was using the Spark interpreter, meaning that the syntax you will see in the following images are Spark code, or Pyspark to be more specific.\r\n\r\nThe following images shows how the data was loaded, and the number of tweets available in the dataset, 177955.\r\n\r\n![load](http://juandes.github.io/thanksgiving-tweets/images/loaddata_numbertweets.png)\r\n\r\nAfter counting the number of tweets, I called the flatMap to get every single word of the corpus, followed by a reduceByKey action to count them - the total number of words is 2145540. Then a new dataframe, made of the words and their frequency, was created. The next image shows the code, followed by two plots of the top 10 words that include the SQL query written to obtain the result. The first image has a comment that says \"get the 20 most common words\", please ignore it (I forgot to remove the comment before taking the screenshots).\r\n\r\n![load](http://juandes.github.io/thanksgiving-tweets/images/word_count.png)\r\n\r\n![load](http://juandes.github.io/thanksgiving-tweets/images/count_1.png)\r\n\r\n![load](http://juandes.github.io/thanksgiving-tweets/images/count_2.png)\r\n\r\nThese are the top 10 words and their frequency\r\n\r\n- **thanksgiving**: 114,556\r\n- **happy**: 103,026\r\n- **rt**: 85,202\r\n- **to**: 59,143\r\n- **the**: 40,918\r\n- **and**: 36,614\r\n- **you**: 31,783\r\n- **a**: 31,105\r\n- **for**: 28,792\r\n- **all**: ... number missing from\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}