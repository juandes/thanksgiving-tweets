{"name":"A look at some Tweets from Thanksgiving 2015","tagline":"A look at some Tweets obtained during Thanksgiving 2015. Analysis done using Apache Zeppelin and the Spark interpreter","body":"# A look at some Tweets from Thanksgiving 2015\r\n\r\n## Overview\r\n***\r\n\r\nTwitter is great. Behind all those tweets, there is an enormous amount of data that holds many interesting facts, and opinions of almost any subject from people all around the world. \r\n\r\nDuring Thanksgiving 2015 (November 26), while everyone was eating turkey, I fired up Spark to capture tweets containing the keyword ‘thanksgiving’.\r\n\r\nThe reason I did this work was because I was interested in exploring the tweets generated during that period of time, principally the tops hashtags, mentions and retweets. Moreover, I wanted to try Apache Zeppelin, a web-based notebook (similar to iPython or Jupyter) for interactive data analytics.\r\n\r\nNote: Because this report has some images, I changed the layout of this page for better visibility of them.\r\n\r\n## The data\r\n***\r\nThe dataset used is made of 177955 tweets obtained on November 26, 2015.\r\n\r\n## Platforms used\r\n***\r\n- Apache Zeppelin and the Spark interpreter\r\n- Spark Streaming\r\n- Pig\r\n\r\n## Report\r\n***\r\n### Obtaining the data\r\n\r\nThe data used for this work was obtained using Spark Streaming, and its Twitter library. The script written captured only the text component of the tweet, in other word, just the tweet itself. After an hour or so of capturing tweets, I ended up with a directory made of many subdirectories that had the tweets. Because of this, a Pig script was written to transfer the content of all these files into a single one. Both of these scripts are on the GitHub repository of this project.\r\n\r\nThis is an example of a single tweet: Macy's Thanksgiving day parade never gets old #HappyThanksgiving\r\n\r\n### The result\r\n\r\nOnce the data was in the wanted format, it was loaded into Zeppelin. Keep in mind that I used the Spark interpreter, meaning that the syntax you will see in the following images are Spark code, or Pyspark to be more specific.\r\n\r\nThe following screenshots (taken from the Zeppelin notebook) shows how the data was loaded, and the number of tweets available in the dataset, 177955.\r\n\r\n![load](http://juandes.github.io/thanksgiving-tweets/images/loaddata_numbertweets.png)\r\n\r\nAfter counting the number of tweets, I executed the `flatMap` action on my data structure ([RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds) to be precise) to get every single word of the corpus as an element of the RDD, followed by a `reduceByKey` action to count them - the total number of words is 2145540. Then a new dataframe, made of the words and their frequency, was created. The next images display the code written to achieve this, and a bar chart and table that presents the top 10 words. This bar chart and table are merely the output of an SQL query (also shown in the images).\r\n\r\n![load](http://juandes.github.io/thanksgiving-tweets/images/word_count.png)\r\n\r\n![load](http://juandes.github.io/thanksgiving-tweets/images/count_1.png)\r\n\r\n![load](http://juandes.github.io/thanksgiving-tweets/images/count_2.png)\r\n\r\nThese are the top 10 words and their frequency\r\n\r\n- **thanksgiving**: 114,556\r\n- **happy**: 103,026\r\n- **rt**: 85,202\r\n- **to**: 59,143\r\n- **the**: 40,918\r\n- **and**: 36,614\r\n- **you**: 31,783\r\n- **a**: 31,105\r\n- **for**: 28,792\r\n- **all**: 28,089\r\n\r\nIs not surprising that the most common words are \"thanksgiving\" and \"happy\".\r\n\r\nSomething really cool about Zeppelin is that you can change the view of the output of an SQL query by just clicking one of the small icons below the query editor. Some of these views include a regular table (as seen on the previous image), a bar chart, a pie chart and others.\r\n\r\nNow that we know what the most common words are, lets do the same but with the hashtags.\r\n\r\nThe next image shows the code used to get the hashtags. Most of the actions performed at this step are similar to those used to find the most common words, the exception in this case is that I used a regex to remove the special characters that follow a hashtag. For example, someone on Twitter might write `#thanksgiving!` as one word, but Twitter does not allow numbers or special characters on the hashtags, so the hashtag is just `#thanksgiving` - the `!` is just a normal character of the tweet.\r\n\r\n![hashtags code](http://juandes.github.io/thanksgiving-tweets/images/hashtags_code.png)\r\n\r\nThese are the top 10 hashtags:\r\n\r\n- **#thanksgiving**: 13,716\r\n- **#happythanksgiving**: 1,871\r\n- **#thankful**: 1,350\r\n- **#american**: 677\r\n- **#revealed**: 672\r\n- **#family**: 657\r\n- **#imthankfulfor**: 504\r\n- **#macysparade**: 504 (call me skeptic but I do not like that there are two hashtags with the same count)\r\n- **#blessed**: 380\r\n- **#turkey**: 359\r\n\r\n![hashtags](http://juandes.github.io/thanksgiving-tweets/images/hashtags.png)\r\n\r\nThere is one hashtag from this list that feels out of place. Do you agree? That is the hashtag `#revealed`. This hashtag belongs to a tweet from an account called `@Drudge_Report` that starts like this \"#REVEALED: What Your #Thanksgiving Feast Does to Your Organs... Avg #American Will Consume 4,500 Cals...\". Mystery solved.\r\n\r\n![revealed](http://juandes.github.io/thanksgiving-tweets/images/revealed.png)\r\n\r\nSo far we have discovered the top words, and the top hashtags, and now it is time to find the most common mentions.\r\n\r\n![mentions](http://juandes.github.io/thanksgiving-tweets/images/mentions.png)\r\n\r\n- **@onedirection**: 8,919\r\n- **@obj_3**: 831\r\n- **@jensenackles**: 783\r\n- **@truthshallbe**: 701\r\n- **@shawnmendes**: 684\r\n- **@harry_styles**: 666\r\n- **@drudge_report_**: 651\r\n- **@sincerelytumblr**: 630\r\n- **@worldstarfunny**: 578\r\n- **@epicurious**: 577\r\n\r\nThe last section of this report is related to the retweets. As I did before with the hashtags and mentions, I looked for the five most common retweets, and the number of time they were reposted.\r\n\r\nThe process behind this was a bit different than we I did before, mostly because I had to check each tweet to verify if it is in fact, a retweet. Thus I called the`map` function on my original dataframe, and checked if the first characters of the tweet are `RT @`. The result of this `map` action is a new dataframe made of a tuple of 3 elements: the tweet, the retweet status, and the length of the tweet (you will see why soon).\r\n\r\n![retweet1](http://juandes.github.io/thanksgiving-tweets/images/retweet_1.png)\r\n\r\nA second dataframe was created with just those tweets that are retweets. Followed by this, they were counted using `reduceByKey`.\r\n\r\n![retweet2](http://juandes.github.io/thanksgiving-tweets/images/retweet_2.png)\r\n\r\n- **RT @onedirection: Happy Thanksgiving to all our US fans!**: 8917\r\n- **RT @OBJ_3: Happy Thanksgiving yall. Have a blessed day!**: 830\r\n- **RT @JensenAckles: Happy Thanksgiving y'all!!!! I love PIE!!!**: 783\r\n- **RT @truthshallbe: Happy Thanksgiving to All. May the Lord be with you, and Bless you All, and the family. God Bless our Troops.**: 701\r\n- **RT @ShawnMendes: GOODMORNING! HAPPY THANKSGIVING! love you guys \\u2764\\ufe0f**: 676\r\n\r\nNote: Some of these retweets had a URL on them and I removed them.\r\n\r\nBONUS! This is why I added the length of the tweets.\r\n\r\n![stats](http://juandes.github.io/thanksgiving-tweets/images/stats.png)\r\n\r\n## Conclusion\r\nTurkey, Macy's Parade, One Direction and good times. This is what was revealed by the tweets I showed in this report. Moreover, I had the chance of trying Zeppelin, which I found really intuitive and easy to use. The scripts, an export of the Zeppelin notebook, and the dataset is available at the repository of this project (link is at the footer of this page).\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}